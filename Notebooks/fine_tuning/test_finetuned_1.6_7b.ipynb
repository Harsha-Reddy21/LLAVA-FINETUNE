{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the model from HuggingFace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, BitsAndBytesConfig, LlavaNextForConditionalGeneration\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:orange\">Load the model</span>**\n",
    "\n",
    "- **<span style=\"color:yellow\">Quantized model: Requires near `8` GB of GPU memory with the configuration provided in the cell below.</span>**\n",
    "- **<span style=\"color:yellow\">Full model: Requires near `24` GB of GPU for inferencing</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "REPO_ID = \"harshareddy21/llava\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c044e086f434534adda6c61ee6fa6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/869 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Projects\\My-Projects\\Trading-AI\\myenv\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Harshavardhan.reddy\\.cache\\huggingface\\hub\\models--harshareddy21--llava. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ef2c652bb3436680e7a77a3ab9e7ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Define quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "# Load the base model with adapters on top\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    REPO_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:orange\">Function for converting the model output to a nicely formatted JSON</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's turn that into JSON\n",
    "import re\n",
    "def token2json(tokens, is_inner_value=False, added_vocab=None):\n",
    "        \"\"\"\n",
    "        Convert a (generated) token sequence into an ordered JSON format.\n",
    "        \"\"\"\n",
    "        if added_vocab is None:\n",
    "            added_vocab = processor.tokenizer.get_added_vocab()\n",
    "\n",
    "        output = {}\n",
    "\n",
    "        while tokens:\n",
    "            start_token = re.search(r\"<s_(.*?)>\", tokens, re.IGNORECASE)\n",
    "            if start_token is None:\n",
    "                break\n",
    "            key = start_token.group(1)\n",
    "            key_escaped = re.escape(key)\n",
    "\n",
    "            end_token = re.search(rf\"</s_{key_escaped}>\", tokens, re.IGNORECASE)\n",
    "            start_token = start_token.group()\n",
    "            if end_token is None:\n",
    "                tokens = tokens.replace(start_token, \"\")\n",
    "            else:\n",
    "                end_token = end_token.group()\n",
    "                start_token_escaped = re.escape(start_token)\n",
    "                end_token_escaped = re.escape(end_token)\n",
    "                content = re.search(\n",
    "                    f\"{start_token_escaped}(.*?){end_token_escaped}\", tokens, re.IGNORECASE | re.DOTALL\n",
    "                )\n",
    "                if content is not None:\n",
    "                    content = content.group(1).strip()\n",
    "                    if r\"<s_\" in content and r\"</s_\" in content:  # non-leaf node\n",
    "                        value = token2json(content, is_inner_value=True, added_vocab=added_vocab)\n",
    "                        if value:\n",
    "                            if len(value) == 1:\n",
    "                                value = value[0]\n",
    "                            output[key] = value\n",
    "                    else:  # leaf nodes\n",
    "                        output[key] = []\n",
    "                        for leaf in content.split(r\"<sep/>\"):\n",
    "                            leaf = leaf.strip()\n",
    "                            if leaf in added_vocab and leaf[0] == \"<\" and leaf[-2:] == \"/>\":\n",
    "                                leaf = leaf[1:-2]  # for categorical special tokens\n",
    "                            output[key].append(leaf)\n",
    "                        if len(output[key]) == 1:\n",
    "                            output[key] = output[key][0]\n",
    "\n",
    "                tokens = tokens[tokens.find(end_token) + len(end_token) :].strip()\n",
    "                if tokens[:6] == r\"<sep/>\":  # non-leaf nodes\n",
    "                    return [output] + token2json(tokens[6:], is_inner_value=True, added_vocab=added_vocab)\n",
    "\n",
    "        if len(output):\n",
    "            return [output] if is_inner_value else output\n",
    "        else:\n",
    "            return [] if is_inner_value else {\"text_sequence\": tokens}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:orange\">Load the validation dataset</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"naver-clova-ix/cord-v2\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:yellow\">Example 1</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = dataset[0][\"image\"]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"[INST] <image>\\nExtract JSON [/INST]\"\n",
    "max_output_token = 256\n",
    "inputs = processor(prompt, image, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "output = model.generate(**inputs, max_new_tokens=max_output_token)\n",
    "response = processor.decode(output[0], skip_special_tokens=True)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:pink\">Model's output vs the ground truth</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "generated_json = token2json(response)\n",
    "print(\"Expected response:\\n\") \n",
    "pprint(json.loads(dataset[0][\"ground_truth\"])[\"gt_parse\"])\n",
    "print()\n",
    "print(\"Converting model's output to JSON:\\n\")\n",
    "pprint(generated_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:yellow\">Example 2</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = dataset[1][\"image\"]\n",
    "prompt = f\"[INST] <image>\\nExtract JSON [/INST]\"\n",
    "max_output_token = 256\n",
    "inputs = processor(prompt, image, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "output = model.generate(**inputs, max_new_tokens=max_output_token)\n",
    "response = processor.decode(output[0], skip_special_tokens=True)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:pink\">Model's output vs the ground truth</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_json = token2json(response)\n",
    "print(\"Expected response:\\n\") \n",
    "pprint(json.loads(dataset[1][\"ground_truth\"])[\"gt_parse\"])\n",
    "print()\n",
    "print(\"Converting model's output to JSON:\\n\")\n",
    "pprint(generated_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:yellow\">Example 3</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = dataset[10][\"image\"]\n",
    "prompt = f\"[INST] <image>\\nExtract JSON [/INST]\"\n",
    "max_output_token = 256\n",
    "inputs = processor(prompt, image, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "output = model.generate(**inputs, max_new_tokens=max_output_token)\n",
    "response = processor.decode(output[0], skip_special_tokens=True)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:pink\">Model's output vs the ground truth</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_json = token2json(response)\n",
    "print(\"Expected response:\\n\") \n",
    "pprint(json.loads(dataset[10][\"ground_truth\"])[\"gt_parse\"])\n",
    "print()\n",
    "print(\"Converting model's output to JSON:\\n\")\n",
    "pprint(generated_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
